{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yue-zhongqi/cartpole_colab/blob/main/cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZauhjPSfX7pI"
      },
      "source": [
        "# Tutorial and Sample Code for Balancing a Pole on a Cart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBiYOoesYMvr"
      },
      "source": [
        "## Installing dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbgnVwZmX5uW",
        "outputId": "7dcfe5ac-b419-4d5d-8305-1ba3c1240df3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in ./.venv/lib/python3.9/site-packages (0.26.2)\n",
            "Requirement already satisfied: tensorflow in ./.venv/lib/python3.9/site-packages (2.19.0)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in ./.venv/lib/python3.9/site-packages (3.9.4)\n",
            "Requirement already satisfied: imageio in ./.venv/lib/python3.9/site-packages (2.37.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in ./.venv/lib/python3.9/site-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.8.0 in ./.venv/lib/python3.9/site-packages (from gym) (8.6.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in ./.venv/lib/python3.9/site-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in ./.venv/lib/python3.9/site-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.venv/lib/python3.9/site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in ./.venv/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.9/site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.9/site-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./.venv/lib/python3.9/site-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in ./.venv/lib/python3.9/site-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in ./.venv/lib/python3.9/site-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.venv/lib/python3.9/site-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.9/site-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.venv/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.9/site-packages (from importlib_metadata>=4.8.0->gym) (3.21.0)\n",
            "Requirement already satisfied: rich in ./.venv/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in ./.venv/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in ./.venv/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.9/site-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.9/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.9/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym tensorflow numpy matplotlib imageio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwKbYeTgbaTA"
      },
      "source": [
        "## Step 1: Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "j6KpgCLGYWmj"
      },
      "outputs": [],
      "source": [
        "# Import libraries for the RL agent and visualization\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "import base64\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_q_network(state_dim, action_dim):\n",
        "    \"\"\"\n",
        "    Creates a neural network to approximate Q-values for the DQN agent.\n",
        "    \n",
        "    Args:\n",
        "        state_dim (int): Size of the state space (4 for CartPole).\n",
        "        action_dim (int): Number of actions (2 for CartPole: left or right).\n",
        "    \n",
        "    Returns:\n",
        "        tf.keras.Model: A neural network model for Q-value estimation.\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(64, activation='relu', input_shape=(state_dim,), \n",
        "                              kernel_initializer='he_uniform'),  # Input layer\n",
        "        tf.keras.layers.Dense(64, activation='relu', \n",
        "                              kernel_initializer='he_uniform'),  # Hidden layer\n",
        "        tf.keras.layers.Dense(action_dim, activation='linear')           # Output layer\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehbqP9CXbmo7"
      },
      "source": [
        "## Define the Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Go12dH4qbwBy"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        \"\"\"Initialize a replay buffer to store experiences.\"\"\"\n",
        "        self.buffer = deque(maxlen=capacity)  # Fixed-size buffer\n",
        "    \n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add an experience tuple to the buffer.\"\"\"\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Randomly sample a batch of experiences for training.\"\"\"\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return (np.array(states, dtype=np.float32),\n",
        "                np.array(actions, dtype=np.int32),\n",
        "                np.array(rewards, dtype=np.float32),\n",
        "                np.array(next_states, dtype=np.float32),\n",
        "                np.array(dones, dtype=np.bool_))\n",
        "    \n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current number of experiences in the buffer.\"\"\"\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XZ9g3xrcAXE"
      },
      "source": [
        "## Define the DQN Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytxvVmLdcRyw",
        "outputId": "1ce07099-492b-4126-a792-8ff12d79c2db"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        \"\"\"Initialize the DQN agent with Q-networks and hyperparameters.\"\"\"\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.replay_buffer = ReplayBuffer(100000)  # Large capacity for stability\n",
        "        self.q_network = create_q_network(state_dim, action_dim)  # Main network\n",
        "        self.target_network = create_q_network(state_dim, action_dim)  # Target network\n",
        "        self.target_network.set_weights(self.q_network.get_weights())  # Sync initially\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)  # Optimizer\n",
        "        self.epsilon = 1.0  # Exploration rate\n",
        "        self.epsilon_min = 0.1  # Minimum exploration rate\n",
        "        self.epsilon_decay = 0.995  # Decay rate for exploration\n",
        "        self.gamma = 0.99  # Discount factor\n",
        "        self.tau = 0.005  # Soft update factor\n",
        "    \n",
        "    def act(self, state, training=True):\n",
        "        \"\"\"Choose an action using an epsilon-greedy policy.\"\"\"\n",
        "        if training and random.random() < self.epsilon:  # Explore\n",
        "            return random.randrange(self.action_dim)\n",
        "        state_tensor = tf.convert_to_tensor([state], dtype=tf.float32)  # Convert to tensor\n",
        "        q_values = self.q_network(state_tensor)  # Predict Q-values\n",
        "        return int(tf.argmax(q_values[0]).numpy())  # Exploit: pick action with max Q-value\n",
        "    \n",
        "    def update(self, batch_size):\n",
        "        \"\"\"Train the Q-network using a batch of experiences.\"\"\"\n",
        "        if len(self.replay_buffer) < batch_size:  # Wait until buffer has enough data\n",
        "            return\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = self.q_network(states)  # Current Q-value predictions\n",
        "            q_values_selected = tf.reduce_sum(q_values * tf.one_hot(actions, self.action_dim), axis=1)\n",
        "            \n",
        "            # Double DQN: Use main network to select actions, target network to evaluate\n",
        "            next_q_values = self.q_network(next_states)\n",
        "            next_actions = tf.argmax(next_q_values, axis=1)\n",
        "            target_q_values = self.target_network(next_states)\n",
        "            max_next_q = tf.gather(target_q_values, next_actions, batch_dims=1)\n",
        "            targets = rewards + self.gamma * max_next_q * (1.0 - tf.cast(dones, tf.float32))\n",
        "            \n",
        "            loss = tf.reduce_mean(tf.square(targets - q_values_selected))  # MSE loss\n",
        "        \n",
        "        gradients = tape.gradient(loss, self.q_network.trainable_variables)  # Compute gradients\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.q_network.trainable_variables))  # Update weights\n",
        "        self.soft_update_target_network()  # Soft update target network\n",
        "    \n",
        "    def soft_update_target_network(self):\n",
        "        \"\"\"Gradually update the target network using a small tau value.\"\"\"\n",
        "        q_weights = self.q_network.get_weights()\n",
        "        target_weights = self.target_network.get_weights()\n",
        "        for i in range(len(q_weights)):\n",
        "            target_weights[i] = (1 - self.tau) * target_weights[i] + self.tau * q_weights[i]\n",
        "        self.target_network.set_weights(target_weights)\n",
        "    \n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Reduce epsilon to shift from exploration to exploitation.\"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVXGWi_Ncfg-"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyqHr9I5cdkX",
        "outputId": "369a0733-e7c0-4bca-b427-5e1c603ff074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1/500, Reward: 31.0, Avg Reward (last 100): 31.00, Epsilon: 0.995\n",
            "Episode 2/500, Reward: 12.0, Avg Reward (last 100): 21.50, Epsilon: 0.990\n",
            "Episode 3/500, Reward: 12.0, Avg Reward (last 100): 18.33, Epsilon: 0.985\n",
            "Episode 4/500, Reward: 17.0, Avg Reward (last 100): 18.00, Epsilon: 0.980\n",
            "Episode 5/500, Reward: 31.0, Avg Reward (last 100): 20.60, Epsilon: 0.975\n",
            "Episode 6/500, Reward: 46.0, Avg Reward (last 100): 24.83, Epsilon: 0.970\n",
            "Episode 7/500, Reward: 10.0, Avg Reward (last 100): 22.71, Epsilon: 0.966\n",
            "Episode 8/500, Reward: 20.0, Avg Reward (last 100): 22.38, Epsilon: 0.961\n",
            "Episode 9/500, Reward: 14.0, Avg Reward (last 100): 21.44, Epsilon: 0.956\n",
            "Episode 10/500, Reward: 12.0, Avg Reward (last 100): 20.50, Epsilon: 0.951\n",
            "Episode 11/500, Reward: 14.0, Avg Reward (last 100): 19.91, Epsilon: 0.946\n",
            "Episode 12/500, Reward: 13.0, Avg Reward (last 100): 19.33, Epsilon: 0.942\n",
            "Episode 13/500, Reward: 12.0, Avg Reward (last 100): 18.77, Epsilon: 0.937\n",
            "Episode 14/500, Reward: 37.0, Avg Reward (last 100): 20.07, Epsilon: 0.932\n",
            "Episode 15/500, Reward: 16.0, Avg Reward (last 100): 19.80, Epsilon: 0.928\n",
            "Episode 16/500, Reward: 13.0, Avg Reward (last 100): 19.38, Epsilon: 0.923\n",
            "Episode 17/500, Reward: 22.0, Avg Reward (last 100): 19.53, Epsilon: 0.918\n",
            "Episode 18/500, Reward: 24.0, Avg Reward (last 100): 19.78, Epsilon: 0.914\n",
            "Episode 19/500, Reward: 79.0, Avg Reward (last 100): 22.89, Epsilon: 0.909\n",
            "Episode 20/500, Reward: 23.0, Avg Reward (last 100): 22.90, Epsilon: 0.905\n",
            "Episode 21/500, Reward: 13.0, Avg Reward (last 100): 22.43, Epsilon: 0.900\n",
            "Episode 22/500, Reward: 18.0, Avg Reward (last 100): 22.23, Epsilon: 0.896\n",
            "Episode 23/500, Reward: 36.0, Avg Reward (last 100): 22.83, Epsilon: 0.891\n",
            "Episode 24/500, Reward: 20.0, Avg Reward (last 100): 22.71, Epsilon: 0.887\n",
            "Episode 25/500, Reward: 13.0, Avg Reward (last 100): 22.32, Epsilon: 0.882\n",
            "Episode 26/500, Reward: 15.0, Avg Reward (last 100): 22.04, Epsilon: 0.878\n",
            "Episode 27/500, Reward: 16.0, Avg Reward (last 100): 21.81, Epsilon: 0.873\n",
            "Episode 28/500, Reward: 21.0, Avg Reward (last 100): 21.79, Epsilon: 0.869\n",
            "Episode 29/500, Reward: 20.0, Avg Reward (last 100): 21.72, Epsilon: 0.865\n",
            "Episode 30/500, Reward: 14.0, Avg Reward (last 100): 21.47, Epsilon: 0.860\n",
            "Episode 31/500, Reward: 28.0, Avg Reward (last 100): 21.68, Epsilon: 0.856\n",
            "Episode 32/500, Reward: 17.0, Avg Reward (last 100): 21.53, Epsilon: 0.852\n",
            "Episode 33/500, Reward: 24.0, Avg Reward (last 100): 21.61, Epsilon: 0.848\n",
            "Episode 34/500, Reward: 9.0, Avg Reward (last 100): 21.24, Epsilon: 0.843\n",
            "Episode 35/500, Reward: 20.0, Avg Reward (last 100): 21.20, Epsilon: 0.839\n",
            "Episode 36/500, Reward: 10.0, Avg Reward (last 100): 20.89, Epsilon: 0.835\n",
            "Episode 37/500, Reward: 12.0, Avg Reward (last 100): 20.65, Epsilon: 0.831\n",
            "Episode 38/500, Reward: 9.0, Avg Reward (last 100): 20.34, Epsilon: 0.827\n",
            "Episode 39/500, Reward: 13.0, Avg Reward (last 100): 20.15, Epsilon: 0.822\n",
            "Episode 40/500, Reward: 17.0, Avg Reward (last 100): 20.07, Epsilon: 0.818\n",
            "Episode 41/500, Reward: 17.0, Avg Reward (last 100): 20.00, Epsilon: 0.814\n",
            "Episode 42/500, Reward: 13.0, Avg Reward (last 100): 19.83, Epsilon: 0.810\n",
            "Episode 43/500, Reward: 32.0, Avg Reward (last 100): 20.12, Epsilon: 0.806\n",
            "Episode 44/500, Reward: 13.0, Avg Reward (last 100): 19.95, Epsilon: 0.802\n",
            "Episode 45/500, Reward: 51.0, Avg Reward (last 100): 20.64, Epsilon: 0.798\n",
            "Episode 46/500, Reward: 12.0, Avg Reward (last 100): 20.46, Epsilon: 0.794\n",
            "Episode 47/500, Reward: 10.0, Avg Reward (last 100): 20.23, Epsilon: 0.790\n",
            "Episode 48/500, Reward: 11.0, Avg Reward (last 100): 20.04, Epsilon: 0.786\n",
            "Episode 49/500, Reward: 11.0, Avg Reward (last 100): 19.86, Epsilon: 0.782\n",
            "Episode 50/500, Reward: 40.0, Avg Reward (last 100): 20.26, Epsilon: 0.778\n",
            "Episode 51/500, Reward: 10.0, Avg Reward (last 100): 20.06, Epsilon: 0.774\n",
            "Episode 52/500, Reward: 18.0, Avg Reward (last 100): 20.02, Epsilon: 0.771\n",
            "Episode 53/500, Reward: 13.0, Avg Reward (last 100): 19.89, Epsilon: 0.767\n",
            "Episode 54/500, Reward: 31.0, Avg Reward (last 100): 20.09, Epsilon: 0.763\n",
            "Episode 55/500, Reward: 13.0, Avg Reward (last 100): 19.96, Epsilon: 0.759\n",
            "Episode 56/500, Reward: 11.0, Avg Reward (last 100): 19.80, Epsilon: 0.755\n",
            "Episode 57/500, Reward: 42.0, Avg Reward (last 100): 20.19, Epsilon: 0.751\n",
            "Episode 58/500, Reward: 19.0, Avg Reward (last 100): 20.17, Epsilon: 0.748\n",
            "Episode 59/500, Reward: 15.0, Avg Reward (last 100): 20.08, Epsilon: 0.744\n",
            "Episode 60/500, Reward: 10.0, Avg Reward (last 100): 19.92, Epsilon: 0.740\n",
            "Episode 61/500, Reward: 13.0, Avg Reward (last 100): 19.80, Epsilon: 0.737\n",
            "Episode 62/500, Reward: 9.0, Avg Reward (last 100): 19.63, Epsilon: 0.733\n",
            "Episode 63/500, Reward: 15.0, Avg Reward (last 100): 19.56, Epsilon: 0.729\n",
            "Episode 64/500, Reward: 17.0, Avg Reward (last 100): 19.52, Epsilon: 0.726\n",
            "Episode 65/500, Reward: 14.0, Avg Reward (last 100): 19.43, Epsilon: 0.722\n",
            "Episode 66/500, Reward: 16.0, Avg Reward (last 100): 19.38, Epsilon: 0.718\n",
            "Episode 67/500, Reward: 18.0, Avg Reward (last 100): 19.36, Epsilon: 0.715\n",
            "Episode 68/500, Reward: 14.0, Avg Reward (last 100): 19.28, Epsilon: 0.711\n",
            "Episode 69/500, Reward: 12.0, Avg Reward (last 100): 19.17, Epsilon: 0.708\n",
            "Episode 70/500, Reward: 15.0, Avg Reward (last 100): 19.11, Epsilon: 0.704\n",
            "Episode 71/500, Reward: 10.0, Avg Reward (last 100): 18.99, Epsilon: 0.701\n",
            "Episode 72/500, Reward: 11.0, Avg Reward (last 100): 18.88, Epsilon: 0.697\n",
            "Episode 73/500, Reward: 16.0, Avg Reward (last 100): 18.84, Epsilon: 0.694\n",
            "Episode 74/500, Reward: 13.0, Avg Reward (last 100): 18.76, Epsilon: 0.690\n",
            "Episode 75/500, Reward: 24.0, Avg Reward (last 100): 18.83, Epsilon: 0.687\n",
            "Episode 76/500, Reward: 10.0, Avg Reward (last 100): 18.71, Epsilon: 0.683\n",
            "Episode 77/500, Reward: 11.0, Avg Reward (last 100): 18.61, Epsilon: 0.680\n",
            "Episode 78/500, Reward: 11.0, Avg Reward (last 100): 18.51, Epsilon: 0.676\n",
            "Episode 79/500, Reward: 12.0, Avg Reward (last 100): 18.43, Epsilon: 0.673\n",
            "Episode 80/500, Reward: 14.0, Avg Reward (last 100): 18.38, Epsilon: 0.670\n",
            "Episode 81/500, Reward: 11.0, Avg Reward (last 100): 18.28, Epsilon: 0.666\n",
            "Episode 82/500, Reward: 16.0, Avg Reward (last 100): 18.26, Epsilon: 0.663\n",
            "Episode 83/500, Reward: 9.0, Avg Reward (last 100): 18.14, Epsilon: 0.660\n",
            "Episode 84/500, Reward: 15.0, Avg Reward (last 100): 18.11, Epsilon: 0.656\n",
            "Episode 85/500, Reward: 18.0, Avg Reward (last 100): 18.11, Epsilon: 0.653\n",
            "Episode 86/500, Reward: 15.0, Avg Reward (last 100): 18.07, Epsilon: 0.650\n",
            "Episode 87/500, Reward: 12.0, Avg Reward (last 100): 18.00, Epsilon: 0.647\n",
            "Episode 88/500, Reward: 15.0, Avg Reward (last 100): 17.97, Epsilon: 0.643\n",
            "Episode 89/500, Reward: 16.0, Avg Reward (last 100): 17.94, Epsilon: 0.640\n",
            "Episode 90/500, Reward: 14.0, Avg Reward (last 100): 17.90, Epsilon: 0.637\n",
            "Episode 91/500, Reward: 12.0, Avg Reward (last 100): 17.84, Epsilon: 0.634\n",
            "Episode 92/500, Reward: 18.0, Avg Reward (last 100): 17.84, Epsilon: 0.631\n",
            "Episode 93/500, Reward: 15.0, Avg Reward (last 100): 17.81, Epsilon: 0.627\n",
            "Episode 94/500, Reward: 9.0, Avg Reward (last 100): 17.71, Epsilon: 0.624\n",
            "Episode 95/500, Reward: 10.0, Avg Reward (last 100): 17.63, Epsilon: 0.621\n",
            "Episode 96/500, Reward: 11.0, Avg Reward (last 100): 17.56, Epsilon: 0.618\n",
            "Episode 97/500, Reward: 14.0, Avg Reward (last 100): 17.53, Epsilon: 0.615\n",
            "Episode 98/500, Reward: 13.0, Avg Reward (last 100): 17.48, Epsilon: 0.612\n",
            "Episode 99/500, Reward: 10.0, Avg Reward (last 100): 17.40, Epsilon: 0.609\n",
            "Episode 100/500, Reward: 17.0, Avg Reward (last 100): 17.40, Epsilon: 0.606\n",
            "Episode 101/500, Reward: 15.0, Avg Reward (last 100): 17.24, Epsilon: 0.603\n",
            "Episode 102/500, Reward: 11.0, Avg Reward (last 100): 17.23, Epsilon: 0.600\n",
            "Episode 103/500, Reward: 17.0, Avg Reward (last 100): 17.28, Epsilon: 0.597\n",
            "Episode 104/500, Reward: 11.0, Avg Reward (last 100): 17.22, Epsilon: 0.594\n",
            "Episode 105/500, Reward: 13.0, Avg Reward (last 100): 17.04, Epsilon: 0.591\n",
            "Episode 106/500, Reward: 13.0, Avg Reward (last 100): 16.71, Epsilon: 0.588\n",
            "Episode 107/500, Reward: 15.0, Avg Reward (last 100): 16.76, Epsilon: 0.585\n",
            "Episode 108/500, Reward: 13.0, Avg Reward (last 100): 16.69, Epsilon: 0.582\n",
            "Episode 109/500, Reward: 9.0, Avg Reward (last 100): 16.64, Epsilon: 0.579\n",
            "Episode 110/500, Reward: 11.0, Avg Reward (last 100): 16.63, Epsilon: 0.576\n",
            "Episode 111/500, Reward: 10.0, Avg Reward (last 100): 16.59, Epsilon: 0.573\n",
            "Episode 112/500, Reward: 9.0, Avg Reward (last 100): 16.55, Epsilon: 0.570\n",
            "Episode 113/500, Reward: 10.0, Avg Reward (last 100): 16.53, Epsilon: 0.568\n",
            "Episode 114/500, Reward: 10.0, Avg Reward (last 100): 16.26, Epsilon: 0.565\n",
            "Episode 115/500, Reward: 13.0, Avg Reward (last 100): 16.23, Epsilon: 0.562\n",
            "Episode 116/500, Reward: 12.0, Avg Reward (last 100): 16.22, Epsilon: 0.559\n",
            "Episode 117/500, Reward: 26.0, Avg Reward (last 100): 16.26, Epsilon: 0.556\n",
            "Episode 118/500, Reward: 26.0, Avg Reward (last 100): 16.28, Epsilon: 0.554\n",
            "Episode 119/500, Reward: 13.0, Avg Reward (last 100): 15.62, Epsilon: 0.551\n",
            "Episode 120/500, Reward: 10.0, Avg Reward (last 100): 15.49, Epsilon: 0.548\n",
            "Episode 121/500, Reward: 14.0, Avg Reward (last 100): 15.50, Epsilon: 0.545\n",
            "Episode 122/500, Reward: 10.0, Avg Reward (last 100): 15.42, Epsilon: 0.543\n",
            "Episode 123/500, Reward: 18.0, Avg Reward (last 100): 15.24, Epsilon: 0.540\n",
            "Episode 124/500, Reward: 31.0, Avg Reward (last 100): 15.35, Epsilon: 0.537\n",
            "Episode 125/500, Reward: 10.0, Avg Reward (last 100): 15.32, Epsilon: 0.534\n",
            "Episode 126/500, Reward: 8.0, Avg Reward (last 100): 15.25, Epsilon: 0.532\n",
            "Episode 127/500, Reward: 27.0, Avg Reward (last 100): 15.36, Epsilon: 0.529\n",
            "Episode 128/500, Reward: 29.0, Avg Reward (last 100): 15.44, Epsilon: 0.526\n",
            "Episode 129/500, Reward: 10.0, Avg Reward (last 100): 15.34, Epsilon: 0.524\n",
            "Episode 130/500, Reward: 12.0, Avg Reward (last 100): 15.32, Epsilon: 0.521\n",
            "Episode 131/500, Reward: 12.0, Avg Reward (last 100): 15.16, Epsilon: 0.519\n",
            "Episode 132/500, Reward: 12.0, Avg Reward (last 100): 15.11, Epsilon: 0.516\n",
            "Episode 133/500, Reward: 16.0, Avg Reward (last 100): 15.03, Epsilon: 0.513\n",
            "Episode 134/500, Reward: 20.0, Avg Reward (last 100): 15.14, Epsilon: 0.511\n",
            "Episode 135/500, Reward: 21.0, Avg Reward (last 100): 15.15, Epsilon: 0.508\n",
            "Episode 136/500, Reward: 11.0, Avg Reward (last 100): 15.16, Epsilon: 0.506\n",
            "Episode 137/500, Reward: 19.0, Avg Reward (last 100): 15.23, Epsilon: 0.503\n",
            "Episode 138/500, Reward: 16.0, Avg Reward (last 100): 15.30, Epsilon: 0.501\n",
            "Episode 139/500, Reward: 11.0, Avg Reward (last 100): 15.28, Epsilon: 0.498\n",
            "Episode 140/500, Reward: 12.0, Avg Reward (last 100): 15.23, Epsilon: 0.496\n",
            "Episode 141/500, Reward: 10.0, Avg Reward (last 100): 15.16, Epsilon: 0.493\n",
            "Episode 142/500, Reward: 11.0, Avg Reward (last 100): 15.14, Epsilon: 0.491\n",
            "Episode 143/500, Reward: 14.0, Avg Reward (last 100): 14.96, Epsilon: 0.488\n",
            "Episode 144/500, Reward: 15.0, Avg Reward (last 100): 14.98, Epsilon: 0.486\n",
            "Episode 145/500, Reward: 24.0, Avg Reward (last 100): 14.71, Epsilon: 0.483\n",
            "Episode 146/500, Reward: 11.0, Avg Reward (last 100): 14.70, Epsilon: 0.481\n",
            "Episode 147/500, Reward: 14.0, Avg Reward (last 100): 14.74, Epsilon: 0.479\n",
            "Episode 148/500, Reward: 12.0, Avg Reward (last 100): 14.75, Epsilon: 0.476\n",
            "Episode 149/500, Reward: 25.0, Avg Reward (last 100): 14.89, Epsilon: 0.474\n",
            "Episode 150/500, Reward: 35.0, Avg Reward (last 100): 14.84, Epsilon: 0.471\n",
            "Episode 151/500, Reward: 11.0, Avg Reward (last 100): 14.85, Epsilon: 0.469\n",
            "Episode 152/500, Reward: 9.0, Avg Reward (last 100): 14.76, Epsilon: 0.467\n",
            "Episode 153/500, Reward: 10.0, Avg Reward (last 100): 14.73, Epsilon: 0.464\n",
            "Episode 154/500, Reward: 28.0, Avg Reward (last 100): 14.70, Epsilon: 0.462\n",
            "Episode 155/500, Reward: 11.0, Avg Reward (last 100): 14.68, Epsilon: 0.460\n",
            "Episode 156/500, Reward: 18.0, Avg Reward (last 100): 14.75, Epsilon: 0.458\n",
            "Episode 157/500, Reward: 48.0, Avg Reward (last 100): 14.81, Epsilon: 0.455\n",
            "Episode 158/500, Reward: 16.0, Avg Reward (last 100): 14.78, Epsilon: 0.453\n",
            "Episode 159/500, Reward: 11.0, Avg Reward (last 100): 14.74, Epsilon: 0.451\n",
            "Episode 160/500, Reward: 35.0, Avg Reward (last 100): 14.99, Epsilon: 0.448\n",
            "Episode 161/500, Reward: 44.0, Avg Reward (last 100): 15.30, Epsilon: 0.446\n",
            "Episode 162/500, Reward: 16.0, Avg Reward (last 100): 15.37, Epsilon: 0.444\n",
            "Episode 163/500, Reward: 11.0, Avg Reward (last 100): 15.33, Epsilon: 0.442\n",
            "Episode 164/500, Reward: 58.0, Avg Reward (last 100): 15.74, Epsilon: 0.440\n",
            "Episode 165/500, Reward: 26.0, Avg Reward (last 100): 15.86, Epsilon: 0.437\n",
            "Episode 166/500, Reward: 80.0, Avg Reward (last 100): 16.50, Epsilon: 0.435\n",
            "Episode 167/500, Reward: 227.0, Avg Reward (last 100): 18.59, Epsilon: 0.433\n",
            "Episode 168/500, Reward: 158.0, Avg Reward (last 100): 20.03, Epsilon: 0.431\n",
            "Episode 169/500, Reward: 164.0, Avg Reward (last 100): 21.55, Epsilon: 0.429\n",
            "Episode 170/500, Reward: 88.0, Avg Reward (last 100): 22.28, Epsilon: 0.427\n",
            "Episode 171/500, Reward: 42.0, Avg Reward (last 100): 22.60, Epsilon: 0.424\n",
            "Episode 172/500, Reward: 138.0, Avg Reward (last 100): 23.87, Epsilon: 0.422\n",
            "Episode 173/500, Reward: 262.0, Avg Reward (last 100): 26.33, Epsilon: 0.420\n",
            "Episode 174/500, Reward: 172.0, Avg Reward (last 100): 27.92, Epsilon: 0.418\n",
            "Episode 175/500, Reward: 208.0, Avg Reward (last 100): 29.76, Epsilon: 0.416\n",
            "Episode 176/500, Reward: 473.0, Avg Reward (last 100): 34.39, Epsilon: 0.414\n",
            "Episode 177/500, Reward: 500.0, Avg Reward (last 100): 39.28, Epsilon: 0.412\n",
            "Episode 178/500, Reward: 91.0, Avg Reward (last 100): 40.08, Epsilon: 0.410\n",
            "Episode 179/500, Reward: 196.0, Avg Reward (last 100): 41.92, Epsilon: 0.408\n",
            "Episode 180/500, Reward: 474.0, Avg Reward (last 100): 46.52, Epsilon: 0.406\n",
            "Episode 181/500, Reward: 269.0, Avg Reward (last 100): 49.10, Epsilon: 0.404\n",
            "Episode 182/500, Reward: 112.0, Avg Reward (last 100): 50.06, Epsilon: 0.402\n",
            "Episode 183/500, Reward: 280.0, Avg Reward (last 100): 52.77, Epsilon: 0.400\n",
            "Episode 184/500, Reward: 500.0, Avg Reward (last 100): 57.62, Epsilon: 0.398\n",
            "Episode 185/500, Reward: 388.0, Avg Reward (last 100): 61.32, Epsilon: 0.396\n",
            "Episode 186/500, Reward: 500.0, Avg Reward (last 100): 66.17, Epsilon: 0.394\n",
            "Episode 187/500, Reward: 500.0, Avg Reward (last 100): 71.05, Epsilon: 0.392\n",
            "Episode 188/500, Reward: 310.0, Avg Reward (last 100): 74.00, Epsilon: 0.390\n",
            "Episode 189/500, Reward: 500.0, Avg Reward (last 100): 78.84, Epsilon: 0.388\n",
            "Episode 190/500, Reward: 433.0, Avg Reward (last 100): 83.03, Epsilon: 0.386\n",
            "Episode 191/500, Reward: 471.0, Avg Reward (last 100): 87.62, Epsilon: 0.384\n",
            "Episode 192/500, Reward: 500.0, Avg Reward (last 100): 92.44, Epsilon: 0.382\n",
            "Episode 193/500, Reward: 500.0, Avg Reward (last 100): 97.29, Epsilon: 0.380\n",
            "Episode 194/500, Reward: 500.0, Avg Reward (last 100): 102.20, Epsilon: 0.378\n",
            "Episode 195/500, Reward: 317.0, Avg Reward (last 100): 105.27, Epsilon: 0.376\n",
            "Episode 196/500, Reward: 317.0, Avg Reward (last 100): 108.33, Epsilon: 0.374\n",
            "Episode 197/500, Reward: 217.0, Avg Reward (last 100): 110.36, Epsilon: 0.373\n",
            "Episode 198/500, Reward: 500.0, Avg Reward (last 100): 115.23, Epsilon: 0.371\n",
            "Episode 199/500, Reward: 226.0, Avg Reward (last 100): 117.39, Epsilon: 0.369\n",
            "Episode 200/500, Reward: 201.0, Avg Reward (last 100): 119.23, Epsilon: 0.367\n",
            "Episode 201/500, Reward: 285.0, Avg Reward (last 100): 121.93, Epsilon: 0.365\n",
            "Episode 202/500, Reward: 319.0, Avg Reward (last 100): 125.01, Epsilon: 0.363\n",
            "Episode 203/500, Reward: 132.0, Avg Reward (last 100): 126.16, Epsilon: 0.361\n",
            "Episode 204/500, Reward: 170.0, Avg Reward (last 100): 127.75, Epsilon: 0.360\n",
            "Episode 205/500, Reward: 428.0, Avg Reward (last 100): 131.90, Epsilon: 0.358\n",
            "Episode 206/500, Reward: 500.0, Avg Reward (last 100): 136.77, Epsilon: 0.356\n",
            "Episode 207/500, Reward: 170.0, Avg Reward (last 100): 138.32, Epsilon: 0.354\n",
            "Episode 208/500, Reward: 305.0, Avg Reward (last 100): 141.24, Epsilon: 0.353\n",
            "Episode 209/500, Reward: 125.0, Avg Reward (last 100): 142.40, Epsilon: 0.351\n",
            "Episode 210/500, Reward: 263.0, Avg Reward (last 100): 144.92, Epsilon: 0.349\n",
            "Episode 211/500, Reward: 500.0, Avg Reward (last 100): 149.82, Epsilon: 0.347\n",
            "Episode 212/500, Reward: 208.0, Avg Reward (last 100): 151.81, Epsilon: 0.346\n",
            "Episode 213/500, Reward: 489.0, Avg Reward (last 100): 156.60, Epsilon: 0.344\n",
            "Episode 214/500, Reward: 334.0, Avg Reward (last 100): 159.84, Epsilon: 0.342\n",
            "Episode 215/500, Reward: 191.0, Avg Reward (last 100): 161.62, Epsilon: 0.340\n",
            "Episode 216/500, Reward: 191.0, Avg Reward (last 100): 163.41, Epsilon: 0.339\n",
            "Episode 217/500, Reward: 53.0, Avg Reward (last 100): 163.68, Epsilon: 0.337\n",
            "Episode 218/500, Reward: 32.0, Avg Reward (last 100): 163.74, Epsilon: 0.335\n",
            "Episode 219/500, Reward: 287.0, Avg Reward (last 100): 166.48, Epsilon: 0.334\n",
            "Episode 220/500, Reward: 320.0, Avg Reward (last 100): 169.58, Epsilon: 0.332\n",
            "Episode 221/500, Reward: 500.0, Avg Reward (last 100): 174.44, Epsilon: 0.330\n",
            "Episode 222/500, Reward: 181.0, Avg Reward (last 100): 176.15, Epsilon: 0.329\n",
            "Episode 223/500, Reward: 268.0, Avg Reward (last 100): 178.65, Epsilon: 0.327\n",
            "Episode 224/500, Reward: 490.0, Avg Reward (last 100): 183.24, Epsilon: 0.325\n",
            "Episode 225/500, Reward: 157.0, Avg Reward (last 100): 184.71, Epsilon: 0.324\n",
            "Episode 226/500, Reward: 286.0, Avg Reward (last 100): 187.49, Epsilon: 0.322\n",
            "Episode 227/500, Reward: 224.0, Avg Reward (last 100): 189.46, Epsilon: 0.321\n",
            "Episode 228/500, Reward: 232.0, Avg Reward (last 100): 191.49, Epsilon: 0.319\n",
            "Episode 229/500, Reward: 500.0, Avg Reward (last 100): 196.39, Epsilon: 0.317\n",
            "Episode 230/500, Reward: 223.0, Avg Reward (last 100): 198.50, Epsilon: 0.316\n",
            "Episode 231/500, Reward: 500.0, Avg Reward (last 100): 203.38, Epsilon: 0.314\n",
            "Episode 232/500, Reward: 260.0, Avg Reward (last 100): 205.86, Epsilon: 0.313\n",
            "Episode 233/500, Reward: 500.0, Avg Reward (last 100): 210.70, Epsilon: 0.311\n",
            "Episode 234/500, Reward: 250.0, Avg Reward (last 100): 213.00, Epsilon: 0.309\n",
            "Episode 235/500, Reward: 262.0, Avg Reward (last 100): 215.41, Epsilon: 0.308\n",
            "Episode 236/500, Reward: 278.0, Avg Reward (last 100): 218.08, Epsilon: 0.306\n",
            "Episode 237/500, Reward: 291.0, Avg Reward (last 100): 220.80, Epsilon: 0.305\n",
            "Episode 238/500, Reward: 77.0, Avg Reward (last 100): 221.41, Epsilon: 0.303\n",
            "Episode 239/500, Reward: 500.0, Avg Reward (last 100): 226.30, Epsilon: 0.302\n",
            "Episode 240/500, Reward: 500.0, Avg Reward (last 100): 231.18, Epsilon: 0.300\n",
            "Episode 241/500, Reward: 80.0, Avg Reward (last 100): 231.88, Epsilon: 0.299\n",
            "Episode 242/500, Reward: 283.0, Avg Reward (last 100): 234.60, Epsilon: 0.297\n",
            "Episode 243/500, Reward: 203.0, Avg Reward (last 100): 236.49, Epsilon: 0.296\n",
            "Episode 244/500, Reward: 195.0, Avg Reward (last 100): 238.29, Epsilon: 0.294\n",
            "Episode 245/500, Reward: 287.0, Avg Reward (last 100): 240.92, Epsilon: 0.293\n",
            "Episode 246/500, Reward: 199.0, Avg Reward (last 100): 242.80, Epsilon: 0.291\n",
            "Episode 247/500, Reward: 297.0, Avg Reward (last 100): 245.63, Epsilon: 0.290\n",
            "Episode 248/500, Reward: 230.0, Avg Reward (last 100): 247.81, Epsilon: 0.288\n",
            "Episode 249/500, Reward: 135.0, Avg Reward (last 100): 248.91, Epsilon: 0.287\n",
            "Episode 250/500, Reward: 255.0, Avg Reward (last 100): 251.11, Epsilon: 0.286\n",
            "Episode 251/500, Reward: 463.0, Avg Reward (last 100): 255.63, Epsilon: 0.284\n",
            "Episode 252/500, Reward: 500.0, Avg Reward (last 100): 260.54, Epsilon: 0.283\n",
            "Episode 253/500, Reward: 285.0, Avg Reward (last 100): 263.29, Epsilon: 0.281\n",
            "Episode 254/500, Reward: 302.0, Avg Reward (last 100): 266.03, Epsilon: 0.280\n",
            "Episode 255/500, Reward: 347.0, Avg Reward (last 100): 269.39, Epsilon: 0.279\n",
            "Episode 256/500, Reward: 261.0, Avg Reward (last 100): 271.82, Epsilon: 0.277\n",
            "Episode 257/500, Reward: 500.0, Avg Reward (last 100): 276.34, Epsilon: 0.276\n",
            "Episode 258/500, Reward: 280.0, Avg Reward (last 100): 278.98, Epsilon: 0.274\n",
            "Episode 259/500, Reward: 255.0, Avg Reward (last 100): 281.42, Epsilon: 0.273\n",
            "Episode 260/500, Reward: 206.0, Avg Reward (last 100): 283.13, Epsilon: 0.272\n",
            "Episode 261/500, Reward: 317.0, Avg Reward (last 100): 285.86, Epsilon: 0.270\n",
            "Episode 262/500, Reward: 339.0, Avg Reward (last 100): 289.09, Epsilon: 0.269\n",
            "Episode 263/500, Reward: 224.0, Avg Reward (last 100): 291.22, Epsilon: 0.268\n",
            "Episode 264/500, Reward: 359.0, Avg Reward (last 100): 294.23, Epsilon: 0.266\n",
            "Episode 265/500, Reward: 195.0, Avg Reward (last 100): 295.92, Epsilon: 0.265\n",
            "Episode 266/500, Reward: 220.0, Avg Reward (last 100): 297.32, Epsilon: 0.264\n",
            "Episode 267/500, Reward: 283.0, Avg Reward (last 100): 297.88, Epsilon: 0.262\n",
            "Episode 268/500, Reward: 215.0, Avg Reward (last 100): 298.45, Epsilon: 0.261\n",
            "Episode 269/500, Reward: 304.0, Avg Reward (last 100): 299.85, Epsilon: 0.260\n",
            "Episode 270/500, Reward: 308.0, Avg Reward (last 100): 302.05, Epsilon: 0.258\n",
            "Episode 271/500, Reward: 258.0, Avg Reward (last 100): 304.21, Epsilon: 0.257\n",
            "Episode 272/500, Reward: 500.0, Avg Reward (last 100): 307.83, Epsilon: 0.256\n",
            "Episode 273/500, Reward: 470.0, Avg Reward (last 100): 309.91, Epsilon: 0.255\n",
            "Episode 274/500, Reward: 29.0, Avg Reward (last 100): 308.48, Epsilon: 0.253\n",
            "Episode 275/500, Reward: 431.0, Avg Reward (last 100): 310.71, Epsilon: 0.252\n",
            "Episode 276/500, Reward: 500.0, Avg Reward (last 100): 310.98, Epsilon: 0.251\n",
            "Episode 277/500, Reward: 500.0, Avg Reward (last 100): 310.98, Epsilon: 0.249\n",
            "Episode 278/500, Reward: 385.0, Avg Reward (last 100): 313.92, Epsilon: 0.248\n",
            "Episode 279/500, Reward: 500.0, Avg Reward (last 100): 316.96, Epsilon: 0.247\n",
            "Episode 280/500, Reward: 298.0, Avg Reward (last 100): 315.20, Epsilon: 0.246\n",
            "Episode 281/500, Reward: 500.0, Avg Reward (last 100): 317.51, Epsilon: 0.245\n",
            "Episode 282/500, Reward: 500.0, Avg Reward (last 100): 321.39, Epsilon: 0.243\n",
            "Episode 283/500, Reward: 281.0, Avg Reward (last 100): 321.40, Epsilon: 0.242\n",
            "Episode 284/500, Reward: 326.0, Avg Reward (last 100): 319.66, Epsilon: 0.241\n",
            "Episode 285/500, Reward: 500.0, Avg Reward (last 100): 320.78, Epsilon: 0.240\n",
            "Episode 286/500, Reward: 342.0, Avg Reward (last 100): 319.20, Epsilon: 0.238\n",
            "Episode 287/500, Reward: 500.0, Avg Reward (last 100): 319.20, Epsilon: 0.237\n",
            "Episode 288/500, Reward: 421.0, Avg Reward (last 100): 320.31, Epsilon: 0.236\n",
            "Episode 289/500, Reward: 235.0, Avg Reward (last 100): 317.66, Epsilon: 0.235\n",
            "Episode 290/500, Reward: 500.0, Avg Reward (last 100): 318.33, Epsilon: 0.234\n",
            "Episode 291/500, Reward: 210.0, Avg Reward (last 100): 315.72, Epsilon: 0.233\n",
            "Episode 292/500, Reward: 392.0, Avg Reward (last 100): 314.64, Epsilon: 0.231\n",
            "Episode 293/500, Reward: 500.0, Avg Reward (last 100): 314.64, Epsilon: 0.230\n",
            "Episode 294/500, Reward: 269.0, Avg Reward (last 100): 312.33, Epsilon: 0.229\n",
            "Episode 295/500, Reward: 500.0, Avg Reward (last 100): 314.16, Epsilon: 0.228\n",
            "Episode 296/500, Reward: 236.0, Avg Reward (last 100): 313.35, Epsilon: 0.227\n",
            "Episode 297/500, Reward: 320.0, Avg Reward (last 100): 314.38, Epsilon: 0.226\n",
            "Episode 298/500, Reward: 220.0, Avg Reward (last 100): 311.58, Epsilon: 0.225\n",
            "Episode 299/500, Reward: 349.0, Avg Reward (last 100): 312.81, Epsilon: 0.223\n",
            "Episode 300/500, Reward: 500.0, Avg Reward (last 100): 315.80, Epsilon: 0.222\n",
            "Episode 301/500, Reward: 252.0, Avg Reward (last 100): 315.47, Epsilon: 0.221\n",
            "Episode 302/500, Reward: 500.0, Avg Reward (last 100): 317.28, Epsilon: 0.220\n",
            "Episode 303/500, Reward: 500.0, Avg Reward (last 100): 320.96, Epsilon: 0.219\n",
            "Episode 304/500, Reward: 500.0, Avg Reward (last 100): 324.26, Epsilon: 0.218\n",
            "Episode 305/500, Reward: 375.0, Avg Reward (last 100): 323.73, Epsilon: 0.217\n",
            "Episode 306/500, Reward: 500.0, Avg Reward (last 100): 323.73, Epsilon: 0.216\n",
            "Episode 307/500, Reward: 370.0, Avg Reward (last 100): 325.73, Epsilon: 0.215\n",
            "Episode 308/500, Reward: 500.0, Avg Reward (last 100): 327.68, Epsilon: 0.214\n",
            "Episode 309/500, Reward: 361.0, Avg Reward (last 100): 330.04, Epsilon: 0.212\n",
            "Episode 310/500, Reward: 500.0, Avg Reward (last 100): 332.41, Epsilon: 0.211\n",
            "Episode 311/500, Reward: 500.0, Avg Reward (last 100): 332.41, Epsilon: 0.210\n",
            "Episode 312/500, Reward: 346.0, Avg Reward (last 100): 333.79, Epsilon: 0.209\n",
            "Episode 313/500, Reward: 500.0, Avg Reward (last 100): 333.90, Epsilon: 0.208\n",
            "Episode 314/500, Reward: 500.0, Avg Reward (last 100): 335.56, Epsilon: 0.207\n",
            "Episode 315/500, Reward: 500.0, Avg Reward (last 100): 338.65, Epsilon: 0.206\n",
            "Episode 316/500, Reward: 500.0, Avg Reward (last 100): 341.74, Epsilon: 0.205\n",
            "Episode 317/500, Reward: 500.0, Avg Reward (last 100): 346.21, Epsilon: 0.204\n",
            "Episode 318/500, Reward: 500.0, Avg Reward (last 100): 350.89, Epsilon: 0.203\n",
            "Episode 319/500, Reward: 500.0, Avg Reward (last 100): 353.02, Epsilon: 0.202\n",
            "Episode 320/500, Reward: 500.0, Avg Reward (last 100): 354.82, Epsilon: 0.201\n",
            "Episode 321/500, Reward: 500.0, Avg Reward (last 100): 354.82, Epsilon: 0.200\n",
            "Episode 322/500, Reward: 500.0, Avg Reward (last 100): 358.01, Epsilon: 0.199\n",
            "Episode 323/500, Reward: 500.0, Avg Reward (last 100): 360.33, Epsilon: 0.198\n",
            "Episode 324/500, Reward: 500.0, Avg Reward (last 100): 360.43, Epsilon: 0.197\n",
            "Episode 325/500, Reward: 500.0, Avg Reward (last 100): 363.86, Epsilon: 0.196\n",
            "Episode 326/500, Reward: 500.0, Avg Reward (last 100): 366.00, Epsilon: 0.195\n",
            "Episode 327/500, Reward: 500.0, Avg Reward (last 100): 368.76, Epsilon: 0.194\n",
            "Episode 328/500, Reward: 500.0, Avg Reward (last 100): 371.44, Epsilon: 0.193\n",
            "Episode 329/500, Reward: 500.0, Avg Reward (last 100): 371.44, Epsilon: 0.192\n",
            "Episode 330/500, Reward: 500.0, Avg Reward (last 100): 374.21, Epsilon: 0.191\n",
            "Episode 331/500, Reward: 500.0, Avg Reward (last 100): 374.21, Epsilon: 0.190\n",
            "Episode 332/500, Reward: 500.0, Avg Reward (last 100): 376.61, Epsilon: 0.189\n",
            "Episode 333/500, Reward: 500.0, Avg Reward (last 100): 376.61, Epsilon: 0.188\n",
            "Episode 334/500, Reward: 392.0, Avg Reward (last 100): 378.03, Epsilon: 0.187\n",
            "Episode 335/500, Reward: 500.0, Avg Reward (last 100): 380.41, Epsilon: 0.187\n",
            "Episode 336/500, Reward: 500.0, Avg Reward (last 100): 382.63, Epsilon: 0.186\n",
            "Episode 337/500, Reward: 500.0, Avg Reward (last 100): 384.72, Epsilon: 0.185\n",
            "Episode 338/500, Reward: 500.0, Avg Reward (last 100): 388.95, Epsilon: 0.184\n",
            "Episode 339/500, Reward: 500.0, Avg Reward (last 100): 388.95, Epsilon: 0.183\n",
            "Episode 340/500, Reward: 500.0, Avg Reward (last 100): 388.95, Epsilon: 0.182\n",
            "Episode 341/500, Reward: 500.0, Avg Reward (last 100): 393.15, Epsilon: 0.181\n",
            "Episode 342/500, Reward: 500.0, Avg Reward (last 100): 395.32, Epsilon: 0.180\n",
            "Episode 343/500, Reward: 500.0, Avg Reward (last 100): 398.29, Epsilon: 0.179\n",
            "Episode 344/500, Reward: 500.0, Avg Reward (last 100): 401.34, Epsilon: 0.178\n",
            "Episode 345/500, Reward: 500.0, Avg Reward (last 100): 403.47, Epsilon: 0.177\n",
            "Episode 346/500, Reward: 500.0, Avg Reward (last 100): 406.48, Epsilon: 0.177\n",
            "Episode 347/500, Reward: 500.0, Avg Reward (last 100): 408.51, Epsilon: 0.176\n",
            "Episode 348/500, Reward: 500.0, Avg Reward (last 100): 411.21, Epsilon: 0.175\n",
            "Episode 349/500, Reward: 500.0, Avg Reward (last 100): 414.86, Epsilon: 0.174\n",
            "Episode 350/500, Reward: 500.0, Avg Reward (last 100): 417.31, Epsilon: 0.173\n",
            "Episode 351/500, Reward: 500.0, Avg Reward (last 100): 417.68, Epsilon: 0.172\n",
            "Episode 352/500, Reward: 500.0, Avg Reward (last 100): 417.68, Epsilon: 0.171\n",
            "Episode 353/500, Reward: 500.0, Avg Reward (last 100): 419.83, Epsilon: 0.170\n",
            "Episode 354/500, Reward: 500.0, Avg Reward (last 100): 421.81, Epsilon: 0.170\n",
            "Episode 355/500, Reward: 500.0, Avg Reward (last 100): 423.34, Epsilon: 0.169\n",
            "Episode 356/500, Reward: 500.0, Avg Reward (last 100): 425.73, Epsilon: 0.168\n",
            "Episode 357/500, Reward: 491.0, Avg Reward (last 100): 425.64, Epsilon: 0.167\n",
            "Episode 358/500, Reward: 500.0, Avg Reward (last 100): 427.84, Epsilon: 0.166\n",
            "Episode 359/500, Reward: 500.0, Avg Reward (last 100): 430.29, Epsilon: 0.165\n",
            "Episode 360/500, Reward: 500.0, Avg Reward (last 100): 433.23, Epsilon: 0.165\n",
            "Episode 361/500, Reward: 500.0, Avg Reward (last 100): 435.06, Epsilon: 0.164\n",
            "Episode 362/500, Reward: 500.0, Avg Reward (last 100): 436.67, Epsilon: 0.163\n",
            "Episode 363/500, Reward: 500.0, Avg Reward (last 100): 439.43, Epsilon: 0.162\n",
            "Episode 364/500, Reward: 500.0, Avg Reward (last 100): 440.84, Epsilon: 0.161\n",
            "Episode 365/500, Reward: 500.0, Avg Reward (last 100): 443.89, Epsilon: 0.160\n",
            "Episode 366/500, Reward: 500.0, Avg Reward (last 100): 446.69, Epsilon: 0.160\n",
            "Episode 367/500, Reward: 500.0, Avg Reward (last 100): 448.86, Epsilon: 0.159\n",
            "Episode 368/500, Reward: 500.0, Avg Reward (last 100): 451.71, Epsilon: 0.158\n",
            "Episode 369/500, Reward: 500.0, Avg Reward (last 100): 453.67, Epsilon: 0.157\n",
            "Episode 370/500, Reward: 500.0, Avg Reward (last 100): 455.59, Epsilon: 0.157\n",
            "Episode 371/500, Reward: 500.0, Avg Reward (last 100): 458.01, Epsilon: 0.156\n",
            "Episode 372/500, Reward: 500.0, Avg Reward (last 100): 458.01, Epsilon: 0.155\n",
            "Episode 373/500, Reward: 500.0, Avg Reward (last 100): 458.31, Epsilon: 0.154\n",
            "Episode 374/500, Reward: 500.0, Avg Reward (last 100): 463.02, Epsilon: 0.153\n",
            "Episode 375/500, Reward: 500.0, Avg Reward (last 100): 463.71, Epsilon: 0.153\n",
            "Episode 376/500, Reward: 500.0, Avg Reward (last 100): 463.71, Epsilon: 0.152\n",
            "Episode 377/500, Reward: 500.0, Avg Reward (last 100): 463.71, Epsilon: 0.151\n",
            "Episode 378/500, Reward: 500.0, Avg Reward (last 100): 464.86, Epsilon: 0.150\n",
            "Episode 379/500, Reward: 500.0, Avg Reward (last 100): 464.86, Epsilon: 0.150\n",
            "Episode 380/500, Reward: 500.0, Avg Reward (last 100): 466.88, Epsilon: 0.149\n",
            "Episode 381/500, Reward: 468.0, Avg Reward (last 100): 466.56, Epsilon: 0.148\n",
            "Episode 382/500, Reward: 500.0, Avg Reward (last 100): 466.56, Epsilon: 0.147\n",
            "Episode 383/500, Reward: 500.0, Avg Reward (last 100): 468.75, Epsilon: 0.147\n",
            "Episode 384/500, Reward: 500.0, Avg Reward (last 100): 470.49, Epsilon: 0.146\n",
            "Episode 385/500, Reward: 500.0, Avg Reward (last 100): 470.49, Epsilon: 0.145\n",
            "Episode 386/500, Reward: 500.0, Avg Reward (last 100): 472.07, Epsilon: 0.144\n",
            "Episode 387/500, Reward: 500.0, Avg Reward (last 100): 472.07, Epsilon: 0.144\n",
            "Episode 388/500, Reward: 500.0, Avg Reward (last 100): 472.86, Epsilon: 0.143\n",
            "Episode 389/500, Reward: 500.0, Avg Reward (last 100): 475.51, Epsilon: 0.142\n",
            "Episode 390/500, Reward: 500.0, Avg Reward (last 100): 475.51, Epsilon: 0.142\n",
            "Episode 391/500, Reward: 500.0, Avg Reward (last 100): 478.41, Epsilon: 0.141\n",
            "Episode 392/500, Reward: 500.0, Avg Reward (last 100): 479.49, Epsilon: 0.140\n",
            "Episode 393/500, Reward: 500.0, Avg Reward (last 100): 479.49, Epsilon: 0.139\n",
            "Episode 394/500, Reward: 500.0, Avg Reward (last 100): 481.80, Epsilon: 0.139\n",
            "Episode 395/500, Reward: 500.0, Avg Reward (last 100): 481.80, Epsilon: 0.138\n",
            "Episode 396/500, Reward: 500.0, Avg Reward (last 100): 484.44, Epsilon: 0.137\n",
            "Episode 397/500, Reward: 500.0, Avg Reward (last 100): 486.24, Epsilon: 0.137\n",
            "Episode 398/500, Reward: 500.0, Avg Reward (last 100): 489.04, Epsilon: 0.136\n",
            "Episode 399/500, Reward: 500.0, Avg Reward (last 100): 490.55, Epsilon: 0.135\n",
            "Episode 400/500, Reward: 500.0, Avg Reward (last 100): 490.55, Epsilon: 0.135\n",
            "Episode 401/500, Reward: 500.0, Avg Reward (last 100): 493.03, Epsilon: 0.134\n",
            "Episode 402/500, Reward: 500.0, Avg Reward (last 100): 493.03, Epsilon: 0.133\n",
            "Episode 403/500, Reward: 500.0, Avg Reward (last 100): 493.03, Epsilon: 0.133\n",
            "Episode 404/500, Reward: 500.0, Avg Reward (last 100): 493.03, Epsilon: 0.132\n",
            "Episode 405/500, Reward: 500.0, Avg Reward (last 100): 494.28, Epsilon: 0.131\n",
            "Episode 406/500, Reward: 500.0, Avg Reward (last 100): 494.28, Epsilon: 0.131\n",
            "Episode 407/500, Reward: 500.0, Avg Reward (last 100): 495.58, Epsilon: 0.130\n",
            "Episode 408/500, Reward: 500.0, Avg Reward (last 100): 495.58, Epsilon: 0.129\n",
            "Episode 409/500, Reward: 500.0, Avg Reward (last 100): 496.97, Epsilon: 0.129\n",
            "Episode 410/500, Reward: 500.0, Avg Reward (last 100): 496.97, Epsilon: 0.128\n",
            "Episode 411/500, Reward: 500.0, Avg Reward (last 100): 496.97, Epsilon: 0.127\n",
            "Episode 412/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.127\n",
            "Episode 413/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.126\n",
            "Episode 414/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.126\n",
            "Episode 415/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.125\n",
            "Episode 416/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.124\n",
            "Episode 417/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.124\n",
            "Episode 418/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.123\n",
            "Episode 419/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.122\n",
            "Episode 420/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.122\n",
            "Episode 421/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.121\n",
            "Episode 422/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.121\n",
            "Episode 423/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.120\n",
            "Episode 424/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.119\n",
            "Episode 425/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.119\n",
            "Episode 426/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.118\n",
            "Episode 427/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.118\n",
            "Episode 428/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.117\n",
            "Episode 429/500, Reward: 500.0, Avg Reward (last 100): 498.51, Epsilon: 0.116\n",
            "Episode 430/500, Reward: 497.0, Avg Reward (last 100): 498.48, Epsilon: 0.116\n",
            "Episode 431/500, Reward: 500.0, Avg Reward (last 100): 498.48, Epsilon: 0.115\n",
            "Episode 432/500, Reward: 500.0, Avg Reward (last 100): 498.48, Epsilon: 0.115\n",
            "Episode 433/500, Reward: 500.0, Avg Reward (last 100): 498.48, Epsilon: 0.114\n",
            "Episode 434/500, Reward: 500.0, Avg Reward (last 100): 499.56, Epsilon: 0.114\n",
            "Episode 435/500, Reward: 500.0, Avg Reward (last 100): 499.56, Epsilon: 0.113\n",
            "Episode 436/500, Reward: 500.0, Avg Reward (last 100): 499.56, Epsilon: 0.112\n",
            "Episode 437/500, Reward: 500.0, Avg Reward (last 100): 499.56, Epsilon: 0.112\n",
            "Episode 438/500, Reward: 500.0, Avg Reward (last 100): 499.56, Epsilon: 0.111\n",
            "Episode 439/500, Reward: 500.0, Avg Reward (last 100): 499.56, Epsilon: 0.111\n",
            "Episode 440/500, Reward: 489.0, Avg Reward (last 100): 499.45, Epsilon: 0.110\n",
            "Episode 441/500, Reward: 500.0, Avg Reward (last 100): 499.45, Epsilon: 0.110\n",
            "Episode 442/500, Reward: 500.0, Avg Reward (last 100): 499.45, Epsilon: 0.109\n",
            "Episode 443/500, Reward: 500.0, Avg Reward (last 100): 499.45, Epsilon: 0.109\n",
            "Episode 444/500, Reward: 500.0, Avg Reward (last 100): 499.45, Epsilon: 0.108\n",
            "Episode 445/500, Reward: 500.0, Avg Reward (last 100): 499.45, Epsilon: 0.107\n",
            "Episode 446/500, Reward: 500.0, Avg Reward (last 100): 499.45, Epsilon: 0.107\n",
            "Episode 447/500, Reward: 500.0, Avg Reward (last 100): 499.45, Epsilon: 0.106\n",
            "Episode 448/500, Reward: 500.0, Avg Reward (last 100): 499.45, Epsilon: 0.106\n",
            "Episode 449/500, Reward: 500.0, Avg Reward (last 100): 499.45, Epsilon: 0.105\n",
            "Episode 450/500, Reward: 500.0, Avg Reward (last 100): 499.45, Epsilon: 0.105\n",
            "Episode 451/500, Reward: 500.0, Avg Reward (last 100): 499.45, Epsilon: 0.104\n",
            "Episode 452/500, Reward: 500.0, Avg Reward (last 100): 499.45, Epsilon: 0.104\n",
            "Episode 453/500, Reward: 500.0, Avg Reward (last 100): 499.45, Epsilon: 0.103\n",
            "Episode 454/500, Reward: 500.0, Avg Reward (last 100): 499.45, Epsilon: 0.103\n",
            "Episode 455/500, Reward: 500.0, Avg Reward (last 100): 499.45, Epsilon: 0.102\n",
            "Episode 456/500, Reward: 500.0, Avg Reward (last 100): 499.45, Epsilon: 0.102\n",
            "Episode 457/500, Reward: 500.0, Avg Reward (last 100): 499.54, Epsilon: 0.101\n",
            "Episode 458/500, Reward: 500.0, Avg Reward (last 100): 499.54, Epsilon: 0.101\n",
            "Episode 459/500, Reward: 500.0, Avg Reward (last 100): 499.54, Epsilon: 0.100\n",
            "Episode 460/500, Reward: 500.0, Avg Reward (last 100): 499.54, Epsilon: 0.100\n",
            "Episode 461/500, Reward: 473.0, Avg Reward (last 100): 499.27, Epsilon: 0.100\n",
            "Episode 462/500, Reward: 500.0, Avg Reward (last 100): 499.27, Epsilon: 0.100\n",
            "Episode 463/500, Reward: 495.0, Avg Reward (last 100): 499.22, Epsilon: 0.100\n",
            "Episode 464/500, Reward: 500.0, Avg Reward (last 100): 499.22, Epsilon: 0.100\n",
            "Episode 465/500, Reward: 500.0, Avg Reward (last 100): 499.22, Epsilon: 0.100\n",
            "Episode 466/500, Reward: 500.0, Avg Reward (last 100): 499.22, Epsilon: 0.100\n",
            "Episode 467/500, Reward: 500.0, Avg Reward (last 100): 499.22, Epsilon: 0.100\n",
            "Episode 468/500, Reward: 500.0, Avg Reward (last 100): 499.22, Epsilon: 0.100\n",
            "Episode 469/500, Reward: 475.0, Avg Reward (last 100): 498.97, Epsilon: 0.100\n",
            "Episode 470/500, Reward: 500.0, Avg Reward (last 100): 498.97, Epsilon: 0.100\n",
            "Episode 471/500, Reward: 484.0, Avg Reward (last 100): 498.81, Epsilon: 0.100\n",
            "Episode 472/500, Reward: 500.0, Avg Reward (last 100): 498.81, Epsilon: 0.100\n",
            "Episode 473/500, Reward: 500.0, Avg Reward (last 100): 498.81, Epsilon: 0.100\n",
            "Episode 474/500, Reward: 500.0, Avg Reward (last 100): 498.81, Epsilon: 0.100\n",
            "Episode 475/500, Reward: 491.0, Avg Reward (last 100): 498.72, Epsilon: 0.100\n",
            "Episode 476/500, Reward: 481.0, Avg Reward (last 100): 498.53, Epsilon: 0.100\n",
            "Episode 477/500, Reward: 496.0, Avg Reward (last 100): 498.49, Epsilon: 0.100\n",
            "Episode 478/500, Reward: 500.0, Avg Reward (last 100): 498.49, Epsilon: 0.100\n",
            "Episode 479/500, Reward: 499.0, Avg Reward (last 100): 498.48, Epsilon: 0.100\n",
            "Episode 480/500, Reward: 483.0, Avg Reward (last 100): 498.31, Epsilon: 0.100\n",
            "Episode 481/500, Reward: 492.0, Avg Reward (last 100): 498.55, Epsilon: 0.100\n",
            "Episode 482/500, Reward: 500.0, Avg Reward (last 100): 498.55, Epsilon: 0.100\n",
            "Episode 483/500, Reward: 467.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Episode 484/500, Reward: 500.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Episode 485/500, Reward: 500.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Episode 486/500, Reward: 500.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Episode 487/500, Reward: 500.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Episode 488/500, Reward: 500.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Episode 489/500, Reward: 500.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Episode 490/500, Reward: 500.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Episode 491/500, Reward: 500.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Episode 492/500, Reward: 500.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Episode 493/500, Reward: 500.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Episode 494/500, Reward: 500.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Episode 495/500, Reward: 500.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Episode 496/500, Reward: 500.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Episode 497/500, Reward: 500.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Episode 498/500, Reward: 500.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Episode 499/500, Reward: 500.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Episode 500/500, Reward: 500.0, Avg Reward (last 100): 498.22, Epsilon: 0.100\n",
            "Training rewards: [31.0, 12.0, 12.0, 17.0, 31.0, 46.0, 10.0, 20.0, 14.0, 12.0, 14.0, 13.0, 12.0, 37.0, 16.0, 13.0, 22.0, 24.0, 79.0, 23.0, 13.0, 18.0, 36.0, 20.0, 13.0, 15.0, 16.0, 21.0, 20.0, 14.0, 28.0, 17.0, 24.0, 9.0, 20.0, 10.0, 12.0, 9.0, 13.0, 17.0, 17.0, 13.0, 32.0, 13.0, 51.0, 12.0, 10.0, 11.0, 11.0, 40.0, 10.0, 18.0, 13.0, 31.0, 13.0, 11.0, 42.0, 19.0, 15.0, 10.0, 13.0, 9.0, 15.0, 17.0, 14.0, 16.0, 18.0, 14.0, 12.0, 15.0, 10.0, 11.0, 16.0, 13.0, 24.0, 10.0, 11.0, 11.0, 12.0, 14.0, 11.0, 16.0, 9.0, 15.0, 18.0, 15.0, 12.0, 15.0, 16.0, 14.0, 12.0, 18.0, 15.0, 9.0, 10.0, 11.0, 14.0, 13.0, 10.0, 17.0, 15.0, 11.0, 17.0, 11.0, 13.0, 13.0, 15.0, 13.0, 9.0, 11.0, 10.0, 9.0, 10.0, 10.0, 13.0, 12.0, 26.0, 26.0, 13.0, 10.0, 14.0, 10.0, 18.0, 31.0, 10.0, 8.0, 27.0, 29.0, 10.0, 12.0, 12.0, 12.0, 16.0, 20.0, 21.0, 11.0, 19.0, 16.0, 11.0, 12.0, 10.0, 11.0, 14.0, 15.0, 24.0, 11.0, 14.0, 12.0, 25.0, 35.0, 11.0, 9.0, 10.0, 28.0, 11.0, 18.0, 48.0, 16.0, 11.0, 35.0, 44.0, 16.0, 11.0, 58.0, 26.0, 80.0, 227.0, 158.0, 164.0, 88.0, 42.0, 138.0, 262.0, 172.0, 208.0, 473.0, 500.0, 91.0, 196.0, 474.0, 269.0, 112.0, 280.0, 500.0, 388.0, 500.0, 500.0, 310.0, 500.0, 433.0, 471.0, 500.0, 500.0, 500.0, 317.0, 317.0, 217.0, 500.0, 226.0, 201.0, 285.0, 319.0, 132.0, 170.0, 428.0, 500.0, 170.0, 305.0, 125.0, 263.0, 500.0, 208.0, 489.0, 334.0, 191.0, 191.0, 53.0, 32.0, 287.0, 320.0, 500.0, 181.0, 268.0, 490.0, 157.0, 286.0, 224.0, 232.0, 500.0, 223.0, 500.0, 260.0, 500.0, 250.0, 262.0, 278.0, 291.0, 77.0, 500.0, 500.0, 80.0, 283.0, 203.0, 195.0, 287.0, 199.0, 297.0, 230.0, 135.0, 255.0, 463.0, 500.0, 285.0, 302.0, 347.0, 261.0, 500.0, 280.0, 255.0, 206.0, 317.0, 339.0, 224.0, 359.0, 195.0, 220.0, 283.0, 215.0, 304.0, 308.0, 258.0, 500.0, 470.0, 29.0, 431.0, 500.0, 500.0, 385.0, 500.0, 298.0, 500.0, 500.0, 281.0, 326.0, 500.0, 342.0, 500.0, 421.0, 235.0, 500.0, 210.0, 392.0, 500.0, 269.0, 500.0, 236.0, 320.0, 220.0, 349.0, 500.0, 252.0, 500.0, 500.0, 500.0, 375.0, 500.0, 370.0, 500.0, 361.0, 500.0, 500.0, 346.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 392.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 491.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 468.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 497.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 489.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 473.0, 500.0, 495.0, 500.0, 500.0, 500.0, 500.0, 500.0, 475.0, 500.0, 484.0, 500.0, 500.0, 500.0, 491.0, 481.0, 496.0, 500.0, 499.0, 483.0, 492.0, 500.0, 467.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "object __array__ method not producing an array",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/IPython/core/formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/backend_bases.py:2204\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2201\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2202\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2203\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2204\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/backend_bases.py:2054\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2052\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2053\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[0;32m-> 2054\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:496\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:444\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    440\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[1;32m    446\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    447\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:387\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    386\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/figure.py:3161\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3158\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   3159\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m-> 3161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3162\u001b[0m mimage\u001b[38;5;241m.\u001b[39m_draw_list_compositing_images(\n\u001b[1;32m   3163\u001b[0m     renderer, \u001b[38;5;28mself\u001b[39m, artists, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppressComposite)\n\u001b[1;32m   3165\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/patches.py:632\u001b[0m, in \u001b[0;36mPatch.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    630\u001b[0m tpath \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform_path_non_affine(path)\n\u001b[1;32m    631\u001b[0m affine \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mget_affine()\n\u001b[0;32m--> 632\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_paths_with_artist_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Work around a bug in the PDF and SVG renderers, which\u001b[39;49;00m\n\u001b[1;32m    636\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# do not draw the hatches if the facecolor is fully\u001b[39;49;00m\n\u001b[1;32m    637\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# transparent, but do if it is None.\u001b[39;49;00m\n\u001b[1;32m    638\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_facecolor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_facecolor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/patches.py:617\u001b[0m, in \u001b[0;36mPatch._draw_paths_with_artist_properties\u001b[0;34m(self, renderer, draw_path_args_list)\u001b[0m\n\u001b[1;32m    614\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m PathEffectRenderer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_path_effects(), renderer)\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m draw_path_args \u001b[38;5;129;01min\u001b[39;00m draw_path_args_list:\n\u001b[0;32m--> 617\u001b[0m     \u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdraw_path_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m gc\u001b[38;5;241m.\u001b[39mrestore()\n\u001b[1;32m    620\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:131\u001b[0m, in \u001b[0;36mRendererAgg.draw_path\u001b[0;34m(self, gc, path, transform, rgbFace)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_renderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgbFace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOverflowError\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m         cant_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
            "\u001b[0;31mValueError\u001b[0m: object __array__ method not producing an array"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def train_dqn():\n",
        "    \"\"\"Train the DQN agent on CartPole-v1 and return the trained agent and environment.\"\"\"\n",
        "    env = gym.make('CartPole-v1')\n",
        "    state_dim = env.observation_space.shape[0]  # 4\n",
        "    action_dim = env.action_space.n  # 2\n",
        "    agent = DQNAgent(state_dim, action_dim)\n",
        "    \n",
        "    num_episodes = 500  # Train for 500 episodes\n",
        "    batch_size = 128  # Batch size for training\n",
        "    rewards_history = []  # Store rewards for plotting\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        if isinstance(state, tuple):  # Handle newer gym versions\n",
        "            state = state[0]\n",
        "        state = np.array(state, dtype=np.float32) / np.array([4.8, 10.0, 0.418, 5.0])  # Normalize\n",
        "        total_reward = 0.0  # Initialize as float\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            action = agent.act(state)  # Choose action\n",
        "            step_result = env.step(action)  # Take action\n",
        "            if len(step_result) == 4:  # Older gym version\n",
        "                next_state, reward, done, _ = step_result\n",
        "            else:  # Newer gym version\n",
        "                next_state, reward, terminated, truncated, _ = step_result\n",
        "                done = terminated or truncated\n",
        "            next_state = np.array(next_state, dtype=np.float32) / np.array([4.8, 10.0, 0.418, 5.0])  # Normalize\n",
        "            reward = float(reward)  # Ensure reward is a float\n",
        "            agent.replay_buffer.push(state, action, reward, next_state, done)  # Store experience\n",
        "            state = next_state\n",
        "            total_reward += reward  # Accumulate reward\n",
        "            agent.update(batch_size)  # Train the network\n",
        "        \n",
        "        agent.decay_epsilon()  # Reduce exploration\n",
        "        rewards_history.append(total_reward)  # Store total reward for the episode\n",
        "        \n",
        "        # Compute running average for monitoring\n",
        "        avg_reward = np.mean(rewards_history[-100:]) if len(rewards_history) >= 100 else np.mean(rewards_history)\n",
        "        print(f\"Episode {episode + 1}/{num_episodes}, Reward: {total_reward}, Avg Reward (last 100): {avg_reward:.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
        "    \n",
        "    # Debug: Check rewards_history for invalid values\n",
        "    print(\"Training rewards:\", rewards_history)\n",
        "    \n",
        "    # Ensure rewards_history contains only numeric values\n",
        "    rewards_history = [float(r) for r in rewards_history]  # Convert to floats\n",
        "    if not all(isinstance(r, (int, float)) for r in rewards_history):\n",
        "        raise ValueError(\"rewards_history contains non-numeric values\")\n",
        "    \n",
        "    # Plot training rewards\n",
        "    plt.plot(range(len(rewards_history)), rewards_history)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Cumulative Reward')\n",
        "    plt.title('Training Progress')\n",
        "    plt.show()\n",
        "    \n",
        "    return agent, env\n",
        "\n",
        "# Train the agent\n",
        "agent, env = train_dqn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Demonstrate Agent Action Selection\n",
        "\n",
        "### Objective\n",
        "Sample a random state from the CartPole environment, input it to the trained agent, and output the chosen action.\n",
        "\n",
        "### Approach\n",
        "- Sample a random state from the environment's observation space.\n",
        "- Normalize the state to match training conditions.\n",
        "- Use the trained agent's greedy policy (training=False) to select an action.\n",
        "- Print the state and action for verification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMr6qAqxdOsm",
        "outputId": "0e35ee3b-15d0-4e1f-926c-194a5a43f830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random State (normalized): [-6.31010036e-01  1.14657400e+36 -6.30370524e-01  5.85730799e+37]\n",
            "Chosen Action: 0\n"
          ]
        }
      ],
      "source": [
        "# Sample a random state and get the agent's action\n",
        "random_state = env.observation_space.sample()  # Random state from CartPole\n",
        "random_state = np.array(random_state, dtype=np.float32) / np.array([4.8, 10.0, 0.418, 5.0])  # Normalize\n",
        "action = agent.act(random_state, training=False)  # Greedy action selection\n",
        "\n",
        "print(\"Random State (normalized):\", random_state)\n",
        "print(\"Chosen Action:\", action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Evaluate Agent Effectiveness\n",
        "\n",
        "### Objective\n",
        "Run the agent for 100 episodes, plot cumulative rewards, and ensure the average reward exceeds 195.\n",
        "\n",
        "### Approach\n",
        "- Run 100 evaluation episodes with the greedy policy.\n",
        "- Record and plot the cumulative reward per episode.\n",
        "- Compute and print the average reward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmfMDvyYdWGk",
        "outputId": "fa12ffd8-6707-4cbc-fb86-aef8484d5d17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation rewards: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "object __array__ method not producing an array",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/IPython/core/formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/backend_bases.py:2204\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2201\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2202\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2203\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2204\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/backend_bases.py:2054\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2052\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2053\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[0;32m-> 2054\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:496\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:444\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    440\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[1;32m    446\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    447\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:387\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    386\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/figure.py:3161\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3158\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   3159\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m-> 3161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3162\u001b[0m mimage\u001b[38;5;241m.\u001b[39m_draw_list_compositing_images(\n\u001b[1;32m   3163\u001b[0m     renderer, \u001b[38;5;28mself\u001b[39m, artists, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppressComposite)\n\u001b[1;32m   3165\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/patches.py:632\u001b[0m, in \u001b[0;36mPatch.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    630\u001b[0m tpath \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform_path_non_affine(path)\n\u001b[1;32m    631\u001b[0m affine \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mget_affine()\n\u001b[0;32m--> 632\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_paths_with_artist_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Work around a bug in the PDF and SVG renderers, which\u001b[39;49;00m\n\u001b[1;32m    636\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# do not draw the hatches if the facecolor is fully\u001b[39;49;00m\n\u001b[1;32m    637\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# transparent, but do if it is None.\u001b[39;49;00m\n\u001b[1;32m    638\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_facecolor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_facecolor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/patches.py:617\u001b[0m, in \u001b[0;36mPatch._draw_paths_with_artist_properties\u001b[0;34m(self, renderer, draw_path_args_list)\u001b[0m\n\u001b[1;32m    614\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m PathEffectRenderer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_path_effects(), renderer)\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m draw_path_args \u001b[38;5;129;01min\u001b[39;00m draw_path_args_list:\n\u001b[0;32m--> 617\u001b[0m     \u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdraw_path_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m gc\u001b[38;5;241m.\u001b[39mrestore()\n\u001b[1;32m    620\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/DSAI/Year 2/Sem 2/SC3000 Artifcial Intelligence/Lab Assignment 1/.venv/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:131\u001b[0m, in \u001b[0;36mRendererAgg.draw_path\u001b[0;34m(self, gc, path, transform, rgbFace)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_renderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgbFace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOverflowError\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m         cant_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
            "\u001b[0;31mValueError\u001b[0m: object __array__ method not producing an array"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average cumulative reward: 500.00\n",
            "Is my agent good enough? True\n"
          ]
        }
      ],
      "source": [
        "def evaluate_agent(agent, env, num_episodes=100):\n",
        "    \"\"\"Evaluate the trained agent over multiple episodes.\"\"\"\n",
        "    eval_rewards = []\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        if isinstance(state, tuple):\n",
        "            state = state[0]\n",
        "        state = np.array(state, dtype=np.float32) / np.array([4.8, 10.0, 0.418, 5.0])  # Normalize\n",
        "        total_reward = 0.0  # Initialize as float\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.act(state, training=False)  # Greedy policy\n",
        "            step_result = env.step(action)\n",
        "            if len(step_result) == 4:\n",
        "                next_state, reward, done, _ = step_result\n",
        "            else:\n",
        "                next_state, reward, terminated, truncated, _ = step_result\n",
        "                done = terminated or truncated\n",
        "            next_state = np.array(next_state, dtype=np.float32) / np.array([4.8, 10.0, 0.418, 5.0])\n",
        "            reward = float(reward)  # Ensure reward is a float\n",
        "            total_reward += reward  # Accumulate reward\n",
        "            state = next_state\n",
        "        eval_rewards.append(total_reward)\n",
        "    \n",
        "    # Debug: Print eval_rewards to check for invalid values\n",
        "    print(\"Evaluation rewards:\", eval_rewards)\n",
        "    \n",
        "    # Ensure eval_rewards contains only numeric values\n",
        "    eval_rewards = [float(r) for r in eval_rewards]  # Convert to floats\n",
        "    if not all(isinstance(r, (int, float)) for r in eval_rewards):\n",
        "        raise ValueError(\"eval_rewards contains non-numeric values\")\n",
        "    \n",
        "    # Plot evaluation rewards\n",
        "    plt.plot(range(len(eval_rewards)), eval_rewards)\n",
        "    plt.title('Cumulative reward for each episode')\n",
        "    plt.ylabel('Cumulative reward')\n",
        "    plt.xlabel('Episode')  # Added for clarity\n",
        "    plt.show()\n",
        "    \n",
        "    # Compute and print average reward in the specified format\n",
        "    avg_reward = np.mean(eval_rewards)\n",
        "    print(f\"Average cumulative reward: {avg_reward:.2f}\")\n",
        "    print(f\"Is my agent good enough? {avg_reward > 195}\")\n",
        "\n",
        "# Run evaluation\n",
        "evaluate_agent(agent, env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Render One Episode\n",
        "\n",
        "### Objective\n",
        "Render a single episode played by the agent as a video in the notebook.\n",
        "\n",
        "### Approach\n",
        "- Create an environment with 'rgb_array' mode for frame capture.\n",
        "- Run one episode, collecting frames.\n",
        "- Convert frames into a video and display it using HTML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVucQVRwf6Jm",
        "outputId": "72f0b7d5-7cb5-4e37-da3d-997c1bfb4641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode completed with total reward: 500.0\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import pygame\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def render_episode(agent, env):\n",
        "    \"\"\"\n",
        "    Render one episode of the CartPole environment using Pygame.\n",
        "    \n",
        "    Args:\n",
        "        agent: The trained RL agent (e.g., DQNAgent) with an `act` method.\n",
        "        env: The CartPole environment instance (unused here, recreated with 'human' mode).\n",
        "    \"\"\"\n",
        "    # Create a new environment instance with 'human' render mode for Pygame\n",
        "    env = gym.make('CartPole-v1', render_mode='human')\n",
        "    \n",
        "    # Reset the environment to start a new episode\n",
        "    state = env.reset()\n",
        "    if isinstance(state, tuple):  # Handle newer gym versions returning tuples\n",
        "        state = state[0]\n",
        "    # Normalize the state to match training conditions\n",
        "    state = np.array(state, dtype=np.float32) / np.array([4.8, 10.0, 0.418, 5.0])\n",
        "    \n",
        "    done = False\n",
        "    total_reward = 0.0\n",
        "    \n",
        "    # Initialize Pygame\n",
        "    pygame.init()\n",
        "    \n",
        "    # Run one episode\n",
        "    while not done:\n",
        "        # Select action using the agent's greedy policy\n",
        "        action = agent.act(state, training=False)\n",
        "        \n",
        "        # Perform the action in the environment\n",
        "        step_result = env.step(action)\n",
        "        if len(step_result) == 4:  # Older gym versions\n",
        "            next_state, reward, done, _ = step_result\n",
        "        else:  # Newer gym versions with terminated/truncated\n",
        "            next_state, reward, terminated, truncated, _ = step_result\n",
        "            done = terminated or truncated\n",
        "        \n",
        "        # Update state and accumulate reward\n",
        "        next_state = np.array(next_state, dtype=np.float32) / np.array([4.8, 10.0, 0.418, 5.0])\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        \n",
        "        # Render the current frame\n",
        "        env.render()\n",
        "        \n",
        "        # Add a delay to control rendering speed (adjustable)\n",
        "        time.sleep(0.02)\n",
        "    \n",
        "    # Cleanup: close the environment and Pygame\n",
        "    env.close()\n",
        "    pygame.quit()\n",
        "    \n",
        "    # Display the total reward for the episode\n",
        "    print(f\"Episode completed with total reward: {total_reward}\")\n",
        "\n",
        "\n",
        "render_episode(agent, env)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNczA+lJQOKjOo4XtzYjWQc",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
